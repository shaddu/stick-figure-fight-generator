{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = 'video_data/stick-figure.gif'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Get FPS\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"FPS: {fps}\")\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe observation : \n",
    "- Ran on few videos and realised that Mediapipe Pose landmarkers are not able to detect a person if the head is not shown in the frame. Likely because of the top down approach of pose detection.\n",
    "- Pose landmarkers are trained for closeup cases, so it is difficult to detect a person from far away in the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip install opencv-python\n",
    "!pip install ipywidgets\n",
    "!pip install matpotlib\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_NAME = 'yolo11s-pose.pt'\n",
    "\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# result = model.track( source='video_data/fight_1.mp4', conf=0.7)\n",
    "result = model.track( source='https://www.youtube.com/watch?v=Sa8gm2F0r8g&ab_channel=SufihanKhanMahabub', conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import json\n",
    "\n",
    "annotated_frames = []\n",
    "keypoints_frames = []\n",
    "keypoints_per_frame = []\n",
    "\n",
    "for index, r in enumerate(result):\n",
    "\n",
    "    height, width = r.orig_shape\n",
    "\n",
    "    blank_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    img = r.plot(img=blank_image, boxes=False)\n",
    "    keypoints_frames.append(img)\n",
    "\n",
    "    img = r.plot(boxes=False)\n",
    "    annotated_frames.append(img)\n",
    "\n",
    "    keypoints_per_frame.append({\"Frame\": index, \"Height\": height,\"Width\": width, \"Keypoints\": r.keypoints.data.detach().cpu().tolist()})\n",
    "\n",
    "fps = 4\n",
    "clip = ImageSequenceClip(keypoints_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"keypoints.gif\", fps=fps)\n",
    "\n",
    "clip = ImageSequenceClip(annotated_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"annotated.gif\", fps=fps)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"keypoints_per_frame.json\"\n",
    "\n",
    "print(f\"Writing keypoints to {file_path}\")\n",
    "\n",
    "# Write data to the JSON file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(keypoints_per_frame, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results\n",
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import torch\n",
    "import json\n",
    "\n",
    "with open('keypoints_per_frame.json', 'r') as f:\n",
    "    frames = json.load(f)\n",
    "\n",
    "keypoints_frames = []\n",
    "\n",
    "for frame in frames:\n",
    "\n",
    "    # Dummy orig image\n",
    "    orig_img = np.zeros((frame[\"Height\"], frame[\"Width\"], 3), dtype=np.uint8)  # Height x Width x Channels\n",
    "\n",
    "    # Required attributes\n",
    "    orig_shape = (orig_img.shape[0], orig_img.shape[1])  # (Height, Width)\n",
    "    boxes = None         # You can fill with real Boxes object\n",
    "    masks = None         # Same for masks\n",
    "    probs = None\n",
    "    keypoints = torch.tensor(frame[\"Keypoints\"])\n",
    "    obb = None\n",
    "    speed = {'preprocess': 0.0, 'inference': 0.0, 'postprocess': 0.0}\n",
    "    names = {0: 'person', 1: 'dog'}\n",
    "    path = 'dummy.jpg'\n",
    "    save_dir = None\n",
    "\n",
    "\n",
    "    if frame[\"Keypoints\"] == [[]]:\n",
    "        keypoints_frames.append(orig_img)\n",
    "    else:\n",
    "        results = Results(\n",
    "            orig_img=orig_img,\n",
    "            boxes=boxes,\n",
    "            masks=masks,\n",
    "            probs=probs,\n",
    "            keypoints=keypoints,\n",
    "            obb=obb,\n",
    "            names=names,\n",
    "            path=path,\n",
    "        )\n",
    "\n",
    "        plotted_img = results.plot()\n",
    "        keypoints_frames.append(plotted_img)\n",
    "\n",
    "fps = 4\n",
    "clip = ImageSequenceClip(keypoints_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"regenrated_keypoints.gif\", fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
