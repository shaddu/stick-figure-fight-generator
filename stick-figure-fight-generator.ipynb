{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = 'video_data/stick-figure.gif'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Get FPS\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"FPS: {fps}\")\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe observation : \n",
    "- Ran on few videos and realised that Mediapipe Pose landmarkers are not able to detect a person if the head is not shown in the frame. Likely because of the top down approach of pose detection.\n",
    "- Pose landmarkers are trained for closeup cases, so it is difficult to detect a person from far away in the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip install opencv-python\n",
    "!pip install ipywidgets\n",
    "!pip install matpotlib\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import json\n",
    "\n",
    "def save_keypoints(result, filename, generate_gif_bool=False):\n",
    "    \"\"\"\n",
    "    Save keypoints and annotated frames to GIF and JSON files.\n",
    "    Args:\n",
    "        result: The result of the YOLO model tracking.\n",
    "    \"\"\"\n",
    "    # Create a list to store the annotated frames and keypoints         \n",
    "\n",
    "    annotated_frames = []\n",
    "    keypoints_frames = []\n",
    "    keypoints_per_frame = []\n",
    "    file_path = \"keypoints\"\n",
    "\n",
    "    print(f\"Number of frames: {len(result)}\")\n",
    "\n",
    "    for index, r in enumerate(result):\n",
    "\n",
    "        height, width = r.orig_shape\n",
    "\n",
    "        blank_image = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        img = r.plot(img=blank_image, boxes=False)\n",
    "        keypoints_frames.append(img)\n",
    "\n",
    "        img = r.plot(boxes=False)\n",
    "        annotated_frames.append(img)\n",
    "        print(f\"Frame: {index}, Keypoints: {r.keypoints.data.shape}\")\n",
    "        keypoints_per_frame.append({\"Frame\": index, \"Height\": height,\"Width\": width, \"Keypoints\": r.keypoints.data.detach().cpu().tolist()})\n",
    "\n",
    "    def generate_gif(fps=4):\n",
    "        # ideal fps is 20-24\n",
    "        fps = 25\n",
    "        clip = ImageSequenceClip(keypoints_frames, fps=fps)  # Adjust fps as needed\n",
    "        clip.write_gif(\"keypoints.gif\", fps=fps)\n",
    "\n",
    "        clip = ImageSequenceClip(annotated_frames, fps=fps)  # Adjust fps as needed\n",
    "        clip.write_gif(\"annotated.gif\", fps=fps)\n",
    "\n",
    "    def save_keypoints(filename, keypoints_per_frame):\n",
    "        # Specify the file path\n",
    "        print(f\"Writing keypoints to {file_path}\\\\{filename}\")\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(file_path, exist_ok=True)\n",
    "        # Write data to the JSON file\n",
    "        with open(os.path.join(file_path, filename), \"w\", encoding=\"utf-8\") as json_file:\n",
    "           json.dump(keypoints_per_frame, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    if generate_gif_bool:\n",
    "        # Generate GIFs\n",
    "        generate_gif(4)\n",
    "\n",
    "    save_keypoints(filename + \".json\", keypoints_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = 'yolo11s-pose.pt'\n",
    "forceCPU = False\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available() and forceCPU is False:\n",
    "    model = YOLO(MODEL_NAME).to('cuda')  # Forces GPU usage\n",
    "else:\n",
    "    model = YOLO(MODEL_NAME)\n",
    "\n",
    "print(f\"Model is using device: {model.device}\")\n",
    "\n",
    "filepaths = [\n",
    "    'https://www.youtube.com/watch?v=hCN9ZPvg1Vg&ab_channel=CheeseYoni',\n",
    "]\n",
    "\n",
    "# result = model.track( source='video_data/fight_1.mp4', conf=0.7)\n",
    "\n",
    "for index, filepath in enumerate(filepaths):\n",
    "    print(f\"Processing URL: {filepath}\")\n",
    "    # Track the video\n",
    "    result = []\n",
    "    try:\n",
    "        results = model.track(source=filepath, conf=0.5, stream=True)\n",
    "        for r in results:\n",
    "            result.append(r)\n",
    "            next(results)  # Skip the next result\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "        continue\n",
    "\n",
    "    save_keypoints(result, str(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results\n",
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import torch\n",
    "import json\n",
    "\n",
    "with open('keypoints_per_frame.json', 'r') as f:\n",
    "    frames = json.load(f)\n",
    "\n",
    "keypoints_frames = []\n",
    "\n",
    "for frame in frames:\n",
    "\n",
    "    # Dummy orig image\n",
    "    orig_img = np.ones((frame[\"Height\"], frame[\"Width\"], 3), dtype=np.uint8)*255  # Height x Width x Channels\n",
    "\n",
    "    # Required attributes\n",
    "    orig_shape = (orig_img.shape[0], orig_img.shape[1])  # (Height, Width)\n",
    "    boxes = None         # You can fill with real Boxes object\n",
    "    masks = None         # Same for masks\n",
    "    probs = None\n",
    "    keypoints = torch.tensor(frame[\"Keypoints\"])\n",
    "    obb = None\n",
    "    speed = {'preprocess': 0.0, 'inference': 0.0, 'postprocess': 0.0}\n",
    "    names = {0: 'person', 1: 'dog'}\n",
    "    path = 'dummy.jpg'\n",
    "    save_dir = None\n",
    "\n",
    "\n",
    "    if frame[\"Keypoints\"] == [[]]:\n",
    "        keypoints_frames.append(orig_img)\n",
    "    else:\n",
    "        results = Results(\n",
    "            orig_img=orig_img,\n",
    "            boxes=boxes,\n",
    "            masks=masks,\n",
    "            probs=probs,\n",
    "            keypoints=keypoints,\n",
    "            obb=obb,\n",
    "            names=names,\n",
    "            path=path,\n",
    "        )\n",
    "\n",
    "        plotted_img = results.plot()\n",
    "        keypoints_frames.append(plotted_img)\n",
    "\n",
    "fps = 4\n",
    "clip = ImageSequenceClip(keypoints_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"regenrated_keypoints.gif\", fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
