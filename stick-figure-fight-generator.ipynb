{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = 'video_data/stick-figure.gif'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Get FPS\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"FPS: {fps}\")\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe observation : \n",
    "- Ran on few videos and realised that Mediapipe Pose landmarkers are not able to detect a person if the head is not shown in the frame. Likely because of the top down approach of pose detection.\n",
    "- Pose landmarkers are trained for closeup cases, so it is difficult to detect a person from far away in the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip install opencv-python\n",
    "!pip install ipywidgets\n",
    "!pip install matpotlib\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_NAME = 'yolo11s-pose.pt'\n",
    "\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# result = model.track( source='video_data/fight_1.mp4', conf=0.7)\n",
    "result = model.track( source='https://www.youtube.com/watch?v=Sa8gm2F0r8g&ab_channel=SufihanKhanMahabub', conf=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import json\n",
    "\n",
    "annotated_frames = []\n",
    "keypoints_frames = []\n",
    "keypoints_per_frame = []\n",
    "\n",
    "for index, r in enumerate(result):\n",
    "\n",
    "    height, width = r.orig_shape\n",
    "\n",
    "    blank_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    img = r.plot(img=blank_image, boxes=False)\n",
    "    keypoints_frames.append(img)\n",
    "\n",
    "    img = r.plot(boxes=False)\n",
    "    annotated_frames.append(img)\n",
    "\n",
    "    keypoints_per_frame.append({\"Frame\": index, \"Keypoints\": r.keypoints.data.detach().cpu().tolist()})\n",
    "\n",
    "fps = 25\n",
    "clip = ImageSequenceClip(keypoints_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"keypoints.gif\", fps=fps)\n",
    "\n",
    "clip = ImageSequenceClip(annotated_frames, fps=fps)  # Adjust fps as needed\n",
    "clip.write_gif(\"annotated.gif\", fps=fps)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"keypoints_per_frame.json\"\n",
    "\n",
    "# Write data to the JSON file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(keypoints_per_frame, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "with open('keypoints_per_frame.json', 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "print(data_list)\n",
    "# Convert list back to tensor\n",
    "tensor_restored = torch.tensor(data_list[0][\"Keypoints\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results, Keypoints\n",
    "import numpy as np\n",
    "\n",
    "# Dummy orig image\n",
    "orig_img = np.zeros((640, 480, 3), dtype=np.uint8)  # Height x Width x Channels\n",
    "\n",
    "# Required attributes\n",
    "orig_shape = (orig_img.shape[0], orig_img.shape[1])  # (Height, Width)\n",
    "boxes = None         # You can fill with real Boxes object\n",
    "masks = None         # Same for masks\n",
    "probs = None\n",
    "keypoints = Keypoints(tensor_restored, orig_shape)\n",
    "obb = None\n",
    "speed = {'preprocess': 0.0, 'inference': 0.0, 'postprocess': 0.0}\n",
    "names = {0: 'person', 1: 'dog'}\n",
    "path = 'dummy.jpg'\n",
    "save_dir = None\n",
    "\n",
    "\n",
    "# Create the Results object\n",
    "results = Results(\n",
    "    orig_img=orig_img,\n",
    "    boxes=boxes,\n",
    "    masks=masks,\n",
    "    probs=probs,\n",
    "    keypoints=tensor_restored,\n",
    "    obb=obb,\n",
    "    speed=speed,\n",
    "    names=names,\n",
    "    path=path,\n",
    ")\n",
    "\n",
    "results.keypoints.data.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Retrieve the plotted image for the 1138th frame\n",
    "img = results.plot()\n",
    "\n",
    "# Convert the image from BGR (OpenCV format) to RGB (matplotlib format)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image inline\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
